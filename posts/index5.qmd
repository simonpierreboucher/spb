---
title: "Least squares estimation"
format: html
editor: visual
categories:
  - Least squares
  - Method by summation
  - Econometrics
author:
  - name: Simon-Pierre Boucher
    orcid: 0000-0002-3756-3937
    email: simon-pierre.boucher.1@ulaval.ca
    affiliations:
      - name: Université Laval
        address: 2325 Rue de Université
        city: Québec
        state: QC 
        postal-code: G1V 0A6
---

In this chapter we present a way to obtain a value for our coefficient of the simple linear regression model. We start with the ordinary least squares estimation method. This value will be found by minimizing the sum of the squared errors of our regression. We can express the error term as follows :

$$\epsilon_i=y_i-\beta_0+\beta_1x_i$$

Then we perform a summation for all the observations of our squared error term going from $1$ to $n$, i.e. the sum of the squared errors $S(\beta_0,\beta_1)$.

$$S(\beta_0,\beta_1)=\sum_{i=1}^n\epsilon_i^2=\sum_{i=1}^n(y_i-\beta_0+\beta_1 x_i)^2$$

Then we minimize $S(\beta_0,\beta_1)$ with respect to $\beta_0$ and $\beta_1$. The partial derivatives with respect to $\beta_0$ is solved as follows.

$$\frac{\partial S(\beta_0,\beta_1)}{\partial \beta_0}=-2\sum_{i=1}^n(y_i-\beta_0-\beta_1 x_i)$$

The partial derivatives with respect to $\beta_1$ is solved as follows.

$$\frac{\partial S(\beta_0,\beta_1)}{\partial \beta_1}=-2\sum_{i=1}^n(y_i-\beta_0-\beta_1 x_i)x_i$$

The first order condition makes it possible to equalize our two partial derivative at 0 and we can thus find the solution.

$$\frac{\partial S(\beta_0,\beta_1)}{\partial \beta_0}=0$$

$$\frac{\partial S(\beta_0,\beta_1)}{\partial \beta_1}=0$$

The solution for the $\beta_0$ estimator is as follows:

$$-2\sum_{i=1}^n(y_i-\beta_0-\beta_1 x_i)=0$$

We can set the following properties :


- $\sum_{i=1}^ny_i=n \overline{y}$
- $\sum_{i=1}^n \beta_0=n \beta_0$
- $\sum_{i=1}^n \beta_1 x_i=n \beta_1 \overline{x}$

$$n\overline{y}-n \beta_0-n \beta_1 \overline{x}=0$$

$$\beta_0=\frac{n\overline{y}-n \beta_1 \overline{x}}{n}$$

Solution for the estimator of $\beta_0$ :

$$\hat{\beta_0}=\overline{y}-\beta_1 \overline{x}$$


The solution for the $\beta_1$ estimator is as follows:

$$-2\sum_{i=1}^n(y_i-\beta_0-\beta_1 x_i)x_i=0$$

$$-2\sum_{i=1}^n(y_i-(\overline{y}-\beta_1 \overline{x})-\beta_1 x_i)x_i=0$$

$$\sum_{i=1}^nx_iy_i-n \overline{y} \overline{x}+n \beta_1 \overline{x}^2-\beta_1\sum_{i=1}^nx_i^2=0$$

$$\sum_{i=1}^nx_iy_i-n \overline{y} \overline{x}=\beta_1\sum_{i=1}^nx_i^2-n \beta_1 \overline{x}^2$$

$$\sum_{i=1}^nx_iy_i-n \overline{y} \overline{x}=\beta_1\left(\sum_{i=1}^nx_i^2-n \overline{x}^2\right)$$

$$\beta_1=\frac{\sum_{i=1}^nx_iy_i-n \overline{y} \overline{x}}{\sum_{i=1}^nx_i^2-n \overline{x}^2}$$

We can set the following properties :

- $\sum_{i=1}^nx_iy_i-n \overline{y} \overline{x}=(x_i-\overline{x})(y_i-\overline{y})$
- $\sum_{i=1}^nx_i^2-n \overline{x}^2=(x_i-\overline{x})^2$

Solution for the estimator of $\beta_1$ : 

$$\hat{\beta_1}=\frac{(x_i-\overline{x})(y_i-\overline{y})}{(x_i-\overline{x})^2}$$







